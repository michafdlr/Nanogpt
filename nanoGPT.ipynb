{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orientiert an Andrej Karpathy's Video [https://www.youtube.com/watch?v=kCc8FmEb1nY] und seinem Repo (https://github.com/karpathy/nanoGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zueignung\n",
      "\n",
      "\n",
      "Ihr naht euch wieder, schwankende Gestalten,\n",
      "Die früh sich einst dem trüben Blick gezeigt.\n",
      "Versuch ich wohl, euch diesmal festzuhalten?\n",
      "Fühl ich mein Herz noch jenem Wahn geneigt?\n",
      "Ihr drängt euch zu! nun gut, so mögt ihr walten,\n",
      "Wie ihr aus Dunst und Nebel um mich steigt;\n",
      "Mein Busen fühlt sich jugendlich erschüttert\n",
      "Vom Zauberhauch, der euren Zug umwittert.\n",
      "\n",
      "Ihr bringt mit euch die Bilder froher Tage,\n",
      "Und manche liebe Schatten steigen auf;\n",
      "Gleich einer alten, halbverklungnen Sage\n",
      "Kommt erste Lieb und Freundschaft mit herauf;\n",
      "Der Schmerz wird neu, es wiederholt die Klage\n",
      "Des Lebens labyrinthisch irren Lauf,\n",
      "Und nennt die Guten, die, um schöne Stunden\n",
      "Vom Glück getäuscht, vor mir hinweggeschwunden.\n",
      "\n",
      "Sie hören nicht die folgenden Gesänge,\n",
      "Die Seelen, denen ich die ersten sang;\n",
      "Zerstoben ist das freundliche Gedränge,\n",
      "Verklungen, ach! der erste Widerklang.\n",
      "Mein Lied ertönt der unbekannten Menge,\n",
      "Ihr Beifall selbst macht meinem Herzen bang,\n",
      "Und was sich sonst an meinem Lied erfr\n",
      "---\n",
      "1.  Akt--Anmutige Gegend\n",
      "\n",
      "Faust auf blumigen Rasen gebettet, ermüdet, unruhig,\n",
      "schlafsuchend.\n",
      "\n",
      "Dämmerung.\n",
      "\n",
      "Geister-Kreis schwebend bewegt, anmutige kleine Gestalten.\n",
      "\n",
      "\n",
      "ARIEL.\n",
      "\n",
      "Gesang von Aolsharfen begleitet.\n",
      "\n",
      "Wenn der Blüten Frühlingsregen\n",
      "Über alle schwebend sinkt,\n",
      "Wenn der Felder grüner Segen\n",
      "Allen Erdgebornen blinkt,\n",
      "Kleiner Elfen Geistergröße\n",
      "Eilet, wo sie helfen kann;\n",
      "Ob er heilig, ob er böse,\n",
      "Jammert sie der Unglücksmann.\n",
      "\n",
      "Die ihr dies Haupt umschwebt im luft'gen Kreise,\n",
      "Erzeigt euch hier nach edler Elfen Weise:\n",
      "Besänftiget des Herzens grimmen Strauß,\n",
      "Entfernt des Vorwurfs glühend bittre Pfeile,\n",
      "Sein Innres reinigt von erlebtem Graus!\n",
      "Vier sind die Pausen nächtiger Weile;\n",
      "Nun ohne Säumen füllt sie freundlich aus!\n",
      "Erst senkt sein Haupt aufs kühle Polster nieder,\n",
      "Dann badet ihn im Tau aus Lethes Flut!\n",
      "Gelenk sind bald die krampferstarrten Glieder,\n",
      "Wenn er gestärkt dem Tag entgegen ruht.\n",
      "Vollbringt der Elfen schönste Pflicht:\n",
      "Gebt ihn zurück dem heiligen Licht!\n",
      "\n",
      "\n",
      "CHOR\n",
      "\n",
      "einzeln, zu \n"
     ]
    }
   ],
   "source": [
    "faust = open(\"goethe_faust.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "faust_2 = open(\"goethe_faust_2.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "print(faust[:1000])\n",
    "print(\"---\")\n",
    "print(faust_2[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187238 292107\n"
     ]
    }
   ],
   "source": [
    "print(len(faust), len(faust_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"'()*+,-./123456:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[abcdefghijklmnopqrstuvwxyzÄÖÜßäöü—’“”﻿\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(faust + faust_2)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir nutzen einen Tokenizer auf Zeichenebene, d.h. es werden nur einzelne Zeichen kodiert. Andere Möglichkeiten wären Tokenizer die Wordsilben oder Worte kodieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i for i,s in enumerate(chars)} # look-up Tabelle, die Zeichen Ziffern zuweist\n",
    "itos = {i:s for s,i in stoi.items()} # look-up Tabelle, die Ziffern Zeichen zuweist\n",
    "itos\n",
    "encode = lambda s: [stoi[l] for l in s] # Kodierungsfunktion\n",
    "decode = lambda i: ''.join([itos[ix] for ix in i]) # Dekodierungsfunktion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 54, 61, 61, 64, 1, 72, 64, 67, 61, 53]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization der beiden Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([187238]) torch.int64\n",
      "tensor([87, 48, 70, 54, 58, 56, 63, 70, 63, 56,  0,  0,  0, 31, 57, 67,  1, 63,\n",
      "        50, 57, 69,  1, 54, 70, 52, 57,  1, 72, 58, 54, 53, 54, 67,  9,  1, 68,\n",
      "        52, 57, 72, 50, 63, 60, 54, 63, 53, 54,  1, 29, 54, 68, 69, 50, 61, 69,\n",
      "        54, 63,  9,  0, 26, 58, 54,  1, 55, 67, 82, 57,  1, 68, 58, 52, 57,  1,\n",
      "        54, 58, 63, 68, 69,  1, 53, 54, 62,  1, 69, 67, 82, 51, 54, 63,  1, 24,\n",
      "        61, 58, 52, 60,  1, 56, 54, 75, 54, 58, 56, 69, 11,  0, 44, 54, 67, 68,\n",
      "        70, 52, 57,  1, 58, 52, 57,  1, 72, 64, 57, 61,  9,  1, 54, 70, 52, 57,\n",
      "         1, 53, 58, 54, 68, 62, 50, 61,  1, 55, 54, 68, 69, 75, 70, 57, 50, 61,\n",
      "        69, 54, 63, 22,  0, 28, 82, 57, 61,  1, 58, 52, 57,  1, 62, 54, 58, 63,\n",
      "         1, 30, 54, 67, 75,  1, 63, 64, 52, 57,  1, 59, 54, 63, 54, 62,  1, 45,\n",
      "        50, 57, 63,  1, 56, 54, 63, 54, 58, 56, 69, 22,  0, 31, 57, 67,  1, 53,\n",
      "        67, 80, 63, 56, 69,  1, 54, 70, 52, 57,  1, 75, 70,  2,  1, 63, 70, 63,\n",
      "         1, 56, 70, 69,  9,  1, 68, 64,  1, 62, 81, 56, 69,  1, 58, 57, 67,  1,\n",
      "        72, 50, 61, 69, 54, 63,  9,  0, 45, 58, 54,  1, 58, 57, 67,  1, 50, 70,\n",
      "        68,  1, 26, 70, 63, 68, 69,  1, 70, 63, 53,  1, 36, 54, 51, 54, 61,  1,\n",
      "        70, 62,  1, 62, 58, 52, 57,  1, 68, 69, 54, 58, 56, 69, 20,  0, 35, 54,\n",
      "        58, 63,  1, 24, 70, 68, 54, 63,  1, 55, 82, 57, 61, 69,  1, 68, 58, 52,\n",
      "        57,  1, 59, 70, 56, 54, 63, 53, 61, 58, 52, 57,  1, 54, 67, 68, 52, 57,\n",
      "        82, 69, 69, 54, 67, 69,  0, 44, 64, 62,  1, 48, 50, 70, 51, 54, 67, 57,\n",
      "        50, 70, 52, 57,  9,  1, 53, 54, 67,  1, 54, 70, 67, 54, 63,  1, 48, 70,\n",
      "        56,  1, 70, 62, 72, 58, 69, 69, 54, 67, 69, 11,  0,  0, 31, 57, 67,  1,\n",
      "        51, 67, 58, 63, 56, 69,  1, 62, 58, 69,  1, 54, 70, 52, 57,  1, 53, 58,\n",
      "        54,  1, 24, 58, 61, 53, 54, 67,  1, 55, 67, 64, 57, 54, 67,  1, 42, 50,\n",
      "        56, 54,  9,  0, 43, 63, 53,  1, 62, 50, 63, 52, 57, 54,  1, 61, 58, 54,\n",
      "        51, 54,  1, 41, 52, 57, 50, 69, 69, 54, 63,  1, 68, 69, 54, 58, 56, 54,\n",
      "        63,  1, 50, 70, 55, 20,  0, 29, 61, 54, 58, 52, 57,  1, 54, 58, 63, 54,\n",
      "        67,  1, 50, 61, 69, 54, 63,  9,  1, 57, 50, 61, 51, 71, 54, 67, 60, 61,\n",
      "        70, 63, 56, 63, 54, 63,  1, 41, 50, 56, 54,  0, 33, 64, 62, 62, 69,  1,\n",
      "        54, 67, 68, 69, 54,  1, 34, 58, 54, 51,  1, 70, 63, 53,  1, 28, 67, 54,\n",
      "        70, 63, 53, 68, 52, 57, 50, 55, 69,  1, 62, 58, 69,  1, 57, 54, 67, 50,\n",
      "        70, 55, 20,  0, 26, 54, 67,  1, 41, 52, 57, 62, 54, 67, 75,  1, 72, 58,\n",
      "        67, 53,  1, 63, 54, 70,  9,  1, 54, 68,  1, 72, 58, 54, 53, 54, 67, 57,\n",
      "        64, 61, 69,  1, 53, 58, 54,  1, 33, 61, 50, 56, 54,  0, 26, 54, 68,  1,\n",
      "        34, 54, 51, 54, 63, 68,  1, 61, 50, 51, 74, 67, 58, 63, 69, 57, 58, 68,\n",
      "        52, 57,  1, 58, 67, 67, 54, 63,  1, 34, 50, 70, 55,  9,  0, 43, 63, 53,\n",
      "         1, 63, 54, 63, 63, 69,  1, 53, 58, 54,  1, 29, 70, 69, 54, 63,  9,  1,\n",
      "        53, 58, 54,  9,  1, 70, 62,  1, 68, 52, 57, 81, 63, 54,  1, 41, 69, 70,\n",
      "        63, 53, 54, 63,  0, 44, 64, 62,  1, 29, 61, 82, 52, 60,  1, 56, 54, 69,\n",
      "        80, 70, 68, 52, 57, 69,  9,  1, 71, 64, 67,  1, 62, 58, 67,  1, 57, 58,\n",
      "        63, 72, 54, 56, 56, 54, 68, 52, 57, 72, 70, 63, 53, 54, 63, 11,  0,  0,\n",
      "        41, 58, 54,  1, 57, 81, 67, 54, 63,  1, 63, 58, 52, 57, 69,  1, 53, 58,\n",
      "        54,  1, 55, 64, 61, 56, 54, 63, 53, 54, 63,  1, 29, 54, 68, 80, 63, 56,\n",
      "        54,  9,  0, 26, 58, 54,  1, 41, 54, 54, 61, 54, 63,  9,  1, 53, 54, 63,\n",
      "        54, 63,  1, 58, 52, 57,  1, 53, 58, 54,  1, 54, 67, 68, 69, 54, 63,  1,\n",
      "        68, 50, 63, 56, 20,  0, 48, 54, 67, 68, 69, 64, 51, 54, 63,  1, 58, 68,\n",
      "        69,  1, 53, 50, 68,  1, 55, 67, 54, 70, 63, 53, 61, 58, 52, 57, 54,  1,\n",
      "        29, 54, 53, 67, 80, 63, 56, 54,  9,  0, 44, 54, 67, 60, 61, 70, 63, 56,\n",
      "        54, 63,  9,  1, 50, 52, 57,  2,  1, 53, 54, 67,  1, 54, 67, 68, 69, 54,\n",
      "         1, 45, 58, 53, 54, 67, 60, 61, 50, 63, 56, 11,  0, 35, 54, 58, 63,  1,\n",
      "        34, 58, 54, 53,  1, 54, 67, 69, 81, 63, 69,  1, 53, 54, 67,  1, 70, 63,\n",
      "        51, 54, 60, 50, 63, 63, 69, 54, 63,  1, 35, 54, 63, 56, 54,  9,  0, 31,\n",
      "        57, 67,  1, 24, 54, 58, 55, 50, 61, 61,  1, 68, 54, 61, 51, 68, 69,  1,\n",
      "        62, 50, 52, 57, 69,  1, 62, 54, 58, 63, 54, 62,  1, 30, 54, 67, 75, 54,\n",
      "        63,  1, 51, 50, 63, 56,  9,  0, 43, 63, 53,  1, 72, 50, 68,  1, 68, 58,\n",
      "        52, 57,  1, 68, 64, 63, 68, 69,  1, 50, 63,  1, 62, 54, 58, 63, 54, 62,\n",
      "         1, 34, 58, 54, 53,  1, 54, 67, 55, 67])\n"
     ]
    }
   ],
   "source": [
    "data_faust = torch.tensor(encode(faust), dtype = torch.long)\n",
    "data_faust_2 = torch.tensor(encode(faust_2), dtype = torch.long)\n",
    "print(data_faust.shape, data_faust.dtype)\n",
    "print(data_faust[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zueignung\n",
      "\n",
      "\n",
      "Ihr naht euch wieder, schwankende Gestalten,\n",
      "Die früh sich einst dem trüben Blick gezeigt.\n",
      "Versuch ich wohl, euch diesmal festzuhalten?\n",
      "Fühl ich mein Herz noch jenem Wahn geneigt?\n",
      "Ihr drängt euch zu! nun gut, so mögt ihr walten,\n",
      "Wie ihr aus Dunst und Nebel um mich steigt;\n",
      "Mein Busen fühlt sich jugendlich erschüttert\n",
      "Vom Zauberhauch, der euren Zug umwittert.\n",
      "\n",
      "Ihr bringt mit euch die Bilder froher Tage,\n",
      "Und manche liebe Schatten steigen auf;\n",
      "Gleich einer alten, halbverklungnen Sage\n",
      "Kommt erste Lieb und Freundschaft mit herauf;\n",
      "Der Schmerz wird neu, es wiederholt die Klage\n",
      "Des Lebens labyrinthisch irren Lauf,\n",
      "Und nennt die Guten, die, um schöne Stunden\n",
      "Vom Glück getäuscht, vor mir hinweggeschwunden.\n",
      "\n",
      "Sie hören nicht die folgenden Gesänge,\n",
      "Die Seelen, denen ich die ersten sang;\n",
      "Zerstoben ist das freundliche Gedränge,\n",
      "Verklungen, ach! der erste Widerklang.\n",
      "Mein Lied ertönt der unbekannten Menge,\n",
      "Ihr Beifall selbst macht meinem Herzen bang,\n",
      "Und was sich sonst an meinem Lied erfr\n"
     ]
    }
   ],
   "source": [
    "print(decode(data_faust[:1000].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spalte Faust-Text in training und validation. Faust 2 wird als Test-split verwendet. Im wesentlichen wollen wir von unserem Modell Faust 2 schreiben lassen :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = int(0.9 * len(data_faust))\n",
    "data_train = data_faust[:train_length]\n",
    "data_val = data_faust[train_length:]\n",
    "data_test = data_faust_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellen verschiedener Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuerst werden wir uns ansehen, wie wir Batches erstellen und was genau in den Modellen vorhergesagt werden soll. \n",
    "Im Wesentlichen werden wir Kontexte verschiedener Länge bis zu einer maximalen Kontext-Länge verwenden, um das nächste Zeichen \n",
    "vorherzusagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = 8\n",
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([87]); Label: 48\n",
      "Context: tensor([87, 48]); Label: 70\n",
      "Context: tensor([87, 48, 70]); Label: 54\n",
      "Context: tensor([87, 48, 70, 54]); Label: 58\n",
      "Context: tensor([87, 48, 70, 54, 58]); Label: 56\n",
      "Context: tensor([87, 48, 70, 54, 58, 56]); Label: 63\n",
      "Context: tensor([87, 48, 70, 54, 58, 56, 63]); Label: 70\n",
      "Context: tensor([87, 48, 70, 54, 58, 56, 63, 70]); Label: 63\n"
     ]
    }
   ],
   "source": [
    "x = data_train[:context_length]\n",
    "y = data_train[1:context_length + 1]\n",
    "for t in range(context_length):\n",
    "    context = x[:t+1]\n",
    "    yt = y[t]\n",
    "    print(f\"Context: {context}; Label: {yt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([4, 8])\n",
      "Inputs: tensor([[58, 52, 57,  1, 57, 54, 67, 54],\n",
      "        [ 1, 51, 54, 67, 54, 58, 69,  9],\n",
      "        [63, 11,  0, 26, 64, 52, 57,  1],\n",
      "        [68,  1, 58, 68, 69,  1, 56, 50]])\n",
      "Label shapes: torch.Size([4, 8])\n",
      "Labels: tensor([[52, 57,  1, 57, 54, 67, 54, 58],\n",
      "        [51, 54, 67, 54, 58, 69,  9,  0],\n",
      "        [11,  0, 26, 64, 52, 57,  1, 56],\n",
      "        [ 1, 58, 68, 69,  1, 56, 50, 67]])\n",
      "Batch 1:\n",
      "Context: tensor([58]); Label: 52\n",
      "Context: tensor([58, 52]); Label: 57\n",
      "Context: tensor([58, 52, 57]); Label: 1\n",
      "Context: tensor([58, 52, 57,  1]); Label: 57\n",
      "Context: tensor([58, 52, 57,  1, 57]); Label: 54\n",
      "Context: tensor([58, 52, 57,  1, 57, 54]); Label: 67\n",
      "Context: tensor([58, 52, 57,  1, 57, 54, 67]); Label: 54\n",
      "Context: tensor([58, 52, 57,  1, 57, 54, 67, 54]); Label: 58\n",
      "-----\n",
      "Batch 2:\n",
      "Context: tensor([1]); Label: 51\n",
      "Context: tensor([ 1, 51]); Label: 54\n",
      "Context: tensor([ 1, 51, 54]); Label: 67\n",
      "Context: tensor([ 1, 51, 54, 67]); Label: 54\n",
      "Context: tensor([ 1, 51, 54, 67, 54]); Label: 58\n",
      "Context: tensor([ 1, 51, 54, 67, 54, 58]); Label: 69\n",
      "Context: tensor([ 1, 51, 54, 67, 54, 58, 69]); Label: 9\n",
      "Context: tensor([ 1, 51, 54, 67, 54, 58, 69,  9]); Label: 0\n",
      "-----\n",
      "Batch 3:\n",
      "Context: tensor([63]); Label: 11\n",
      "Context: tensor([63, 11]); Label: 0\n",
      "Context: tensor([63, 11,  0]); Label: 26\n",
      "Context: tensor([63, 11,  0, 26]); Label: 64\n",
      "Context: tensor([63, 11,  0, 26, 64]); Label: 52\n",
      "Context: tensor([63, 11,  0, 26, 64, 52]); Label: 57\n",
      "Context: tensor([63, 11,  0, 26, 64, 52, 57]); Label: 1\n",
      "Context: tensor([63, 11,  0, 26, 64, 52, 57,  1]); Label: 56\n",
      "-----\n",
      "Batch 4:\n",
      "Context: tensor([68]); Label: 1\n",
      "Context: tensor([68,  1]); Label: 58\n",
      "Context: tensor([68,  1, 58]); Label: 68\n",
      "Context: tensor([68,  1, 58, 68]); Label: 69\n",
      "Context: tensor([68,  1, 58, 68, 69]); Label: 1\n",
      "Context: tensor([68,  1, 58, 68, 69,  1]); Label: 56\n",
      "Context: tensor([68,  1, 58, 68, 69,  1, 56]); Label: 50\n",
      "Context: tensor([68,  1, 58, 68, 69,  1, 56, 50]); Label: 67\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = data_train if split == \"train\" else data_val if split == \"val\" else data_test\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i + context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+context_length+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "print(f\"Inputs shape: {xb.shape}\")\n",
    "print(f\"Inputs: {xb}\")\n",
    "\n",
    "print(f\"Label shapes: {yb.shape}\")\n",
    "print(f\"Labels: {yb}\")\n",
    "\n",
    "for b in range(xb.shape[0]):\n",
    "    print(f\"Batch {b+1}:\")\n",
    "    for t in range(context_length):\n",
    "        context = xb[b, :t+1]\n",
    "        yt = yb[b, t]\n",
    "        print(f\"Context: {context}; Label: {yt}\")\n",
    "    print(5*\"-\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigramm Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "class BigrammModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 88])\n"
     ]
    }
   ],
   "source": [
    "m = BigrammModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "\n",
    "print(out.shape) # (batch_size, context_length, vocab_size) = (B, T, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
